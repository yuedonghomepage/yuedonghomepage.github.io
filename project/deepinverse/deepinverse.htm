<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="content-type" />
	<title>Deep Inverse Rendering for High-resolution SVBRDF Estimation from an Arbitrary Number of Images</title>
	<link rel="stylesheet" type="text/css" href="../../main.css" media="screen, print" /> 
</head>
<body class="PageContainer">
<div class="PaperPage">
	<div class="PublishInfo">
			Published in
			<span class="BookTitle">ACM Transactions on Graphics, Volume 38, Issue 4 (SIGGRAPH 2019)</span>
	</div>
	<div class="TitleBar">
	<h1>Deep Inverse Rendering for High-resolution SVBRDF Estimation from an Arbitrary Number of Images</h1>
	</div>
	<div class="AuthorBar">
	<ul class="AuthorList">
			<li>
				<span class="Author"><a href="https://gao-duan.github.io/" target="_blank">
				Duan Gao</a><sup>1,2,*</sup></span>
			</li>
			<li>
				<span class="Author"><a href="http://home.ustc.edu.cn/~pableeto/" target="_blank">
				Xiao Li</a><sup>3,2,*</sup></span>
			</li>
			<li>
				<span class="Author"><a href="http://yuedong.shading.me/" target="_blank">
				Yue Dong</a><sup>2</sup>
			</li>
			<li>
				<span class="Author"><a href="http://www.cs.wm.edu/~ppeers/" target="_blank">
				Pieter Peers</a><sup>4</sup></span>
			</li>
			<li>
				<span class="Author"><a href="https://cg.cs.tsinghua.edu.cn/people/~kun/" target="_blank">
				Kun Xu</a><sup>1</sup></span>
			</li>
			<li>
				<span class="Author"><a href="http://research.microsoft.com/users/xtong/xtong.html" target="_blank">
				Xin Tong</a><sup>2</sup></span>
			</li>
		</ul>

		<ul class="AuthorList">
			<li>
				<span class="Affiliation"><sup>1</sup>Tsinghua University</span>
			</li>
			<li>
				<span class="Affiliation"><sup>2</sup>Microsof Research Asia</span>
			</li>
			<li>
				<span class="Affiliation"><sup>3</sup>University of Science and Technology of China</span>
			</li>
			<li>
				<span class="Affiliation"><sup>4</sup> College of William & Mary</span>
			</li>
		</ul>   
	</div>
	
	<div class="TeaserBar">
	<img src="teaser.jpg" alt="rendering resutls" class="PaperFigure" />
	</div>
    <p class="FigureDescription">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Visualizations under natural lighting of four captured 1 k resolution SVBRDFs estimated using our deep inverse rendering framework. The leather 
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;material (left) is reconstructed from just 2 input photographs captured with a mobile phone camera and flash, while the other materials are &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;recovered from 20 input photographs.</p>
  	       
	<div class="AbstractBar">
	<h2 class="PaperSectionTitle">Abstract</h2>
    <p class="Abstract">
In this paper we present a unified deep inverse rendering framework
for estimating the spatially-varying appearance properties of a planar
exemplar from an arbitrary number of input photographs, ranging from
just a single photograph to many photographs.  The precision of the
estimated appearance scales from plausible when the input photographs
fails to capture all the reflectance information, to accurate for
large input sets.  A key distinguishing feature of our framework is
that it directly optimizes for the appearance parameters in a latent
embedded space of spatially-varying appearance, such that no
handcrafted heuristics are needed to regularize the optimization. This
latent embedding is learned through a fully convolutional auto-encoder
that has been designed to regularize the optimization.  Our framework
not only supports an arbitrary number of input photographs, but also
at high resolution.  We demonstrate and evaluate our deep inverse
rendering solution on a wide variety of publicly available datasets.

 </p>
   </div>

    
    <table class="TwoColumnPaper">
      <tr>
        <td>

    <p class="Keywords">*The first two authors contributed equally to this paper.</p>

   	<h2 class="PaperSectionTitle">Keywords</h2>
    <p class="Keywords">Appearance modeling, Inverse rendering</p>

    <h2 class="PaperSectionTitle">Paper and video</h2>
    
    <div class="PaperDownloadList">
        <ul>
			<a href="deepinverse.pdf">
            <li>
                <span class="FileTitle">Paper</span>
                <span class="FileDesc">.pdf | 83.6 MB</span>             
            </li>
			</a>
        </ul>
    </div>
    <h2 class="PaperSectionTitle">Trained model and code</h2>

    <div class="PaperDownloadList">
        <ul>
			<a href="https://github.com/msraig/DeepInverseRendering">
            <li>
                <span class="FileTitle">GitHub Repo</span>
                <span class="FileDesc">Code and model</span>             
            </li>
			</a>
        </ul>
    </div>

    </td>
    <td>
	<h2 class="PaperSectionTitle">Acknowledgements</h2>
    <p class="Acknowledgements">We would like to thank the reviewers for their constructive feedback.
We also thank Baining Guo for discussions and suggestions. Pieter
Peers was partially supported by NSF grant IIS-1350323 and gifts
from Google, Activision, and Nvidia. Duan Gao and Kun Xu are
supported by the National Natural Science Foundation of China
(Project Numbers: 61822204, 61521002).
</p>



	</td>
	</tr>
	</table>

</div>
</body>
</html>
